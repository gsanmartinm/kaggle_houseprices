---
title: "House Prices Analysis"
author: "minetoportconsulting"
date: "07-07-2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# House Prices Prediction

The analysis will be performed in the following steps:
+ load libraries that are going to be required to perform data analysis (wrangling)
+ import the master database (train and test datasets)
+ clean and transform the data
+ perform an EDA, to identify main drivers that explain how the prices are set
+ generate a prediction on the test dataset

It looks pretty obvious, but as a good practice the report should always starts with a short definition of the next steps.

## Libraries required
```{r, echo = FALSE}
library(dplyr)
library(ggplot2)
library(tidyverse)
library(ggthemes)
```
## Importing data
```{r}
train <- read.csv("./data/train.csv")
test <- read.csv("./data/test.csv")
```

## Data cleaning
```{r}
# We should start the analysis by joining both tables.
dim(test)
dim(train)
# train df has 1 column more than test df. This is due to the "SalePrice" column, which has to be incorporated into the test dataset filled with NAs. Also, it is expected that every row incorporates an id that let us differenciate between test and train dataset.
test$SalePrice <- NA
test$dataset <- "test"
train$dataset <- "train"
house_prices <- rbind(test,train)

summary(house_prices)

# EDA : an exploratory analysis must be performed to identify the main components that defines the saleprice.
# for this purposes, train dataset will be used and the final model will be applied on the test dataset.
# LotArea and MSZoning seems to be a good pair of variables to start this analysis

table_by_MSZoning <- train %>% group_by(MSZoning) %>% summarize(p = n()) %>% mutate(pct = round(p/sum(p)*100))

# RL (Residential low density) + RM (Residential medium density) represents 86 pct of the train data. The analysis should start by those groups.

# Is there any correlation between SalePrice and LotArea by Neighborhood?

tabla_RL_by_neighb <- train %>% filter(MSZoning == "RL") %>% group_by(Neighborhood) %>% summarize(p = n()) %>% mutate(pct = round(p/sum(p)*100))

train_RL <- train %>% filter(MSZoning == "RL")

nbhd_trainRL <- as.array(levels(train_RL$Neighborhood))

# calcular correlaciones por cada neighborhood

correl <- data.frame(r_adj = c(1:25), Neighborhood = NA)
for (i in c(1:25)) {
  test_new <- train %>% filter(Neighborhood == nbhd_trainRL[i]) %>% select(Neighborhood,SalePrice,LotArea)
  correl$r_adj[i] <- cor(test_new$SalePrice,test_new$LotArea)
  correl$Neighborhood[i] <- nbhd_trainRL[i]
}

correl %>% ggplot(aes(Neighborhood,r_adj))+
  geom_bar(stat = "identity")+
  coord_flip()

# se puede depurar mas, es un buen quiebre.
# hacer ejercicio con RM y despuÃ©s automatizar en for anidados
# 

train %>% ggplot(aes(log(LotArea), SalePrice))+
  geom_point(size = 1, aes( color = MSZoning))+
  geom_smooth(method = "lm",color = "black")+
  facet_grid(rows = vars(MSZoning), cols = vars(Neighborhood))


```

